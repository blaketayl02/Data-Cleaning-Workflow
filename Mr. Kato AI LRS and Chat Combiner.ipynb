{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf755e-81e7-43aa-8d02-747c3b7fdd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pytz\n",
    "import nltk\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50967feb-ccf9-448a-95d7-2be430ebcb4b",
   "metadata": {},
   "source": [
    "# Import Chat Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0dabd9-cafc-4829-8334-56c38da3d407",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import chatdata from Apex here\n",
    "chatdata = pd.read_csv(\"2025-08-01_kato_messages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee94b8b-e90d-4ac9-ab15-d3a7f9fa45c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dc579d-5ffe-47b3-af5f-55548cd61d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort by CONVERSATION_ID and TIME to maintain order\n",
    "chatdata_sorted = chatdata.sort_values(by=[\"CONVERSATION_ID\", \"TIME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55be9f8f-4102-4297-9e20-9be5afdccf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatdata_sorted[\"TIME\"] = pd.to_datetime(chatdata_sorted[\"TIME\"], format=\"%d-%b-%y %I.%M.%S.%f %p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36225549-844a-40b7-95e7-6dbf879d1bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatdata_sorted[\"USER\"] = chatdata_sorted[\"MESSAGE_ID\"].str.split(\"_\").str[0]\n",
    "chatdata_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64436492-7786-4989-8aa5-57725a0bbc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatdata_sorted[chatdata_sorted[\"USER\"] == \"ffd106a9-3ae4-419d-b0d4-85ebb75e0df2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f61279c-2f42-4aca-b554-87817c39fd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_json_string(json_str):\n",
    "    #remove any unexpected characters around JSON fields\n",
    "    json_str = re.sub(r'(\\w+)\\s*\"', r'\\1\"', json_str)\n",
    "    return json_str\n",
    "\n",
    "def safe_json_loads(json_str):\n",
    "    try:\n",
    "        # Clean the JSON string first\n",
    "        cleaned_str = clean_json_string(json_str)\n",
    "        return json.loads(cleaned_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        print(f\"Problematic JSON string: {json_str[:500]}...\")\n",
    "        return None\n",
    "\n",
    "#read in the raw LRS data here\n",
    "df = pd.read_csv('kato_raw_July.csv')\n",
    "\n",
    "#parse the JSON column\n",
    "df['parsed_json'] = df['Payload'].apply(safe_json_loads)\n",
    "\n",
    "#remove any rows where parsing failed\n",
    "df = df.dropna(subset=['parsed_json'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ae3e37-791a-44bb-a5ac-295f43922eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks that process did not remove any rows from original csv\n",
    "len(df['parsed_json']), len(df['Payload'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e134599-73bd-4d8b-8043-32f95fba6488",
   "metadata": {},
   "source": [
    "# Anonymize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a76a296-dcf0-4817-86d0-4ff71ee8116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set for unique email-name combinations\n",
    "unique_combinations = set()\n",
    "\n",
    "for json_obj in df['parsed_json']:\n",
    "   if json_obj and 'actor' in json_obj:\n",
    "       email = json_obj['actor'].get('mbox')\n",
    "       name = json_obj['actor'].get('name')\n",
    "       if email and name:\n",
    "           combo = (email, name)\n",
    "           unique_combinations.add(combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2877e9-9266-4c11-8573-ae3d2f0f0097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map emails to anonymous identifiers\n",
    "email_mapping = {}\n",
    "counter = 1\n",
    "\n",
    "def anonymize_by_email(json_obj):\n",
    "   global counter\n",
    "   \n",
    "   if json_obj and 'actor' in json_obj:\n",
    "       original_email = json_obj['actor'].get('mbox')\n",
    "       \n",
    "       # If this email hasn't been mapped yet, create a new mapping\n",
    "       if original_email and original_email not in email_mapping:\n",
    "           email_mapping[original_email] = f\"Person {counter}\"\n",
    "           counter += 1\n",
    "       \n",
    "       # Replace the email with the anonymous identifier\n",
    "       if original_email:\n",
    "           json_obj['actor']['mbox'] = email_mapping[original_email]\n",
    "           \n",
    "   return json_obj\n",
    "\n",
    "# Apply the anonymization\n",
    "df['parsed_json'] = df['parsed_json'].apply(anonymize_by_email)\n",
    "\n",
    "# Print the mapping (optional, to keep track of which email maps to which person)\n",
    "print(\"Email to Person mapping:\")\n",
    "for email, person in email_mapping.items():\n",
    "   print(f\"{email} -> {person}\")\n",
    "\n",
    "# Print total number of unique people\n",
    "print(f\"\\nTotal number of unique people: {len(email_mapping)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f35659a-68d8-4e28-8a85-99b4b50cdec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to maintain consistent name mapping\n",
    "name_mapping = {}\n",
    "counter = 1\n",
    "\n",
    "def anonymize_name(json_obj):\n",
    "    global counter\n",
    "    \n",
    "    if json_obj and 'actor' in json_obj:\n",
    "        original_name = json_obj['actor'].get('name')\n",
    "        \n",
    "        # If this name hasn't been mapped yet, create a new mapping\n",
    "        if original_name and original_name not in name_mapping:\n",
    "            name_mapping[original_name] = f\"Person {counter}\"\n",
    "            counter += 1\n",
    "        \n",
    "        # Replace the name with the anonymous version\n",
    "        if original_name:\n",
    "            json_obj['actor']['name'] = name_mapping[original_name]\n",
    "            \n",
    "        # Remove email\n",
    "        if 'mbox' in json_obj['actor']:\n",
    "            del json_obj['actor']['mbox']\n",
    "            \n",
    "    return json_obj\n",
    "\n",
    "# Apply the anonymization\n",
    "df['parsed_json'] = df['parsed_json'].apply(anonymize_name)\n",
    "\n",
    "# Print the mapping (optional, to keep track of who is who)\n",
    "print(\"Name mapping:\", name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4faf37e-fe4d-4aa7-90c6-84eb2a4cf660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the name mapping dictionary to a DataFrame\n",
    "mapping_df = pd.DataFrame.from_dict(name_mapping, orient='index', columns=['Anonymous_ID'])\n",
    "mapping_df.index.name = 'Original_Name'\n",
    "\n",
    "# Save to CSV\n",
    "mapping_df.to_csv('name_mapping_July25_Kato.csv')\n",
    "\n",
    "# Print to verify\n",
    "print(\"Name mapping saved to 'name_mapping.csv'\")\n",
    "print(\"\\nPreview of the mapping DataFrame:\")\n",
    "print(mapping_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0028b9ca-f3aa-4b2c-af67-527d9fda440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract registrations\n",
    "registrations = df['parsed_json'].apply(lambda x: \n",
    "    x.get('context', {}).get('registration') if x else None\n",
    ")\n",
    "\n",
    "# Print the registrations\n",
    "print(\"Registrations:\", registrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab22be3-fb15-4c36-a8b1-21d7f8d21036",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"parsed_json\"].to_csv('anonymized_data_July25_Kato.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e421791f-33e8-44cd-acff-9d26ba4223b6",
   "metadata": {},
   "source": [
    "## Editing LRS Data and Parsing JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f9db3b-e8e9-4869-976c-4800da66b648",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df = pd.read_csv(\"anonymized_data_July25_Kato.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7222c55a-b0e7-405c-85aa-6da30922c607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean JSON and parse data\n",
    "def clean_and_parse_json(x):\n",
    "    if pd.isnull(x):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        #replace PHP symbol with single quote\n",
    "        x = x.replace(\"â€™\", \"'\")\n",
    "\n",
    "        #replace \"True\" with \"true\"\n",
    "        x = x.replace(\"True\", \"true\").replace(\"False\", \"false\").replace(\"None\", \"null\")\n",
    "\n",
    "        #replace single quotes with double quotes (this is going to throw errors for stuff like \"Mr. Kato's\" with the single quote. Go into Excel and Replace \"Mr. Kato's\" -> \"Mr. Katos\" along with other words too\n",
    "        #reload the csv, then run this window again until all errors below are gone. Read carefully, as it will tell you where the error is.. e.g. \"Mr. Kato\"s\"\n",
    "        #there might be an easier work around for this, as it is quite tedious\n",
    "        x = x.replace(\"'\", '\"')\n",
    "\n",
    "        #white space removal\n",
    "        x = x.strip()\n",
    "\n",
    "        #parse cleaned JSON\n",
    "        return json.loads(x)\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        #log rows where it isn't parsing correctly\n",
    "        print(f\"JSONDecodeError: {e} for row: {x}\")\n",
    "        return None\n",
    "\n",
    "#apply cleaning and parsing function to dataset column\n",
    "normalized_df[\"parsed_json\"] = normalized_df[\"parsed_json\"].apply(clean_and_parse_json)\n",
    "\n",
    "#create column indicating whether the row was parsed correctly (good for debugging)\n",
    "normalized_df[\"is_valid_json\"] = normalized_df[\"parsed_json\"].apply(lambda x: isinstance(x, dict))\n",
    "\n",
    "#normalize data into columns based on features\n",
    "normalized_df = pd.json_normalize(normalized_df[\"parsed_json\"].where(normalized_df[\"is_valid_json\"], None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fb566d-aa9e-46fd-857d-48834a71796b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert timestamp to datetime if it isn't already\n",
    "normalized_df[\"timestamp\"] = pd.to_datetime(normalized_df[\"timestamp\"])\n",
    "\n",
    "#check if timestamp (does not have a timezone)\n",
    "if normalized_df[\"timestamp\"].dt.tz is None:\n",
    "    #localize to UTC first\n",
    "    normalized_df[\"timestamp\"] = normalized_df[\"timestamp\"].dt.tz_localize(\"UTC\")\n",
    "\n",
    "#convert to GMT-8 (Etc/GMT+8 corresponds to GMT-8)\n",
    "gmt_8 = pytz.timezone(\"Etc/GMT+8\")\n",
    "normalized_df[\"timestamp_gmt8\"] = normalized_df[\"timestamp\"].dt.tz_convert(gmt_8)\n",
    "\n",
    "#ensure timestamp_gmt8 remains timezone-aware (remove the unnecessary pd.to_datetime())\n",
    "normalized_df = normalized_df[normalized_df[\"timestamp_gmt8\"].notna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffd24e9-9a81-4a5f-9ac7-5e927a4649e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#manually setting the slides to keys with chronological order\n",
    "slide_mapping = {\n",
    "    \"Virtual Patient Case: Mr. Kato\": 1,\n",
    "    \"Introduction\": 2,\n",
    "    \"Learning Objectives\": 3,\n",
    "    \"Case Overview\": 4,\n",
    "    \"Reflecting Back\": 5,\n",
    "    \"Ruling Out a Diagnosis\": 6,\n",
    "    \"1.6 Ruling Out a Diagnosis - Life Threatening Conditions\": 7,\n",
    "    \"1.6 Ruling Out a Diagnosis - Sight Threatening Conditions\": 8,\n",
    "    \"Meeting the Patient\": 9,\n",
    "    \"Confidence Check I: Engage With Mr. Kato\": 10,\n",
    "    \"Practice: Conversational History Taking\": 11,\n",
    "    \"Feedback Survey\": 12,\n",
    "    \"Choose a Path: Hx or DDx\": 13,\n",
    "    \"History Taking\": 14,\n",
    "    \"Mr. Katos History\": 15,\n",
    "    \"Mr. Katos Ocular History\": 16,\n",
    "    \"Practice: Refractive Error Classification\": 17,\n",
    "    \"Refractive Error Classification\": 18,\n",
    "    \"Medical and Social History\": 19,\n",
    "    \"Identifying Vision Loss\": 20,\n",
    "    \"Initial Differential Diagnosis\": 21,\n",
    "    \"Initial DDX: Vascular Occlusion\": 22,\n",
    "    \"Initial DDX: Diabetic Retinopathy\": 23,\n",
    "    \"Initial DDX: Cornea Ulcer\": 24,\n",
    "    \"Initial DDX: Optic Neuritis\": 25,\n",
    "    \"Initial DDX: GCA\": 26,\n",
    "    \"Initial DDX: Cataract\": 27,\n",
    "    \"Initial DDX: AMD\": 28,\n",
    "    \"Initial DDX: Corneal Scarring\": 29,\n",
    "    \"Initial DDX: Retinal Detachment\": 30,\n",
    "    \"Eye Examination\": 31,\n",
    "    \"1.20 Eye Examintation - Visual Acuity\": 32,\n",
    "    \"1.20 Eye Examintation - Pupils\": 33,\n",
    "    \"1.20 Eye Examintation - Confrontational Visual Fields\": 34,\n",
    "    \"1.20 Eye Examintation - Extraocular Movements\": 35,\n",
    "    \"1.20 Eye Examintation - Slit Lamp\": 36,\n",
    "    \"1.20 Eye Examintation - Direct Ophthalmoscope\": 37,\n",
    "    \"Mr. Katos Eye Exam\": 38,\n",
    "    \"1.21 Mr Katos Eye Exam - Slit Lamp of Mild Cataract\": 39,\n",
    "    \"1.21 Mr Katos Eye Exam - Terminology\": 40,\n",
    "    \"1.21 Mr Katos Eye Exam - Snellen Chart\": 41,\n",
    "    \"Direct Ophthalmoscopy\": 42,\n",
    "    \"Direct Ophthalmoscopy: In Practice\": 43,\n",
    "    \"Dilated Fundus Anatomy\": 44,\n",
    "    \"Review: Dilated Fundus Anatomy\": 45,\n",
    "    \"Mr. Kato’s Dilated Fundus Exam\": 46,\n",
    "    \"Confidence Check II: Differential Diagnosis vs. Chatting with Mr. Kato\": 47,\n",
    "    \"Differential Diagnosis\": 48,\n",
    "    \"Patient Communication\": 49,\n",
    "    \"Patient Referral\": 50,\n",
    "    \"Practice: Matching Treatments\": 51,\n",
    "    \"Epilogue\": 52,\n",
    "    \"Learning Review\": 53,\n",
    "}\n",
    "\n",
    "#creating slide number column, mapped to appropriate title\n",
    "normalized_df[\"slide_number\"] = normalized_df[\"object.definition.name.und\"].map(slide_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5fd4a4-f7c2-40ba-a069-e891f3cbc4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    #if response is one character or punctuation, then classify it as \"one_character\"\n",
    "    if len(text.strip()) == 1 or text.strip() in string.punctuation:\n",
    "        return [\"one_character\"]\n",
    "\n",
    "    #convert text to lowercase, remove punctuation\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "#create new column as \"None\"\n",
    "normalized_df.loc[:, \"processed_response\"] = None\n",
    "\n",
    "#ensure the verb is \"answered\" and the description is either \"essay\" or \"text entry interaction\" when preprocessing\n",
    "is_essay_or_text = normalized_df.loc[(normalized_df[\"object.definition.description.und\"] == \"Essay\") | (normalized_df[\"object.definition.description.und\"] == \"Text Entry Interaction\")]\n",
    "\n",
    "#apply preprocessing to the relevant rows only\n",
    "normalized_df.loc[is_essay_or_text.index, \"processed_response\"] = is_essay_or_text[\"result.response\"].apply(preprocess_text)\n",
    "\n",
    "#ensure all other columns remain unchanged in normalized_df\n",
    "normalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37b4a4f-63d6-47cb-bdb6-83db17511bda",
   "metadata": {},
   "source": [
    "## Removing Non Students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a0e69e-8c0f-4c6a-99ba-71cde5c65265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the name mapping file we made above to see which names are team members, then remove them here.\n",
    "\n",
    "normalized_df = normalized_df[~normalized_df[\"actor.name\"].isin([\"Person 10\", \"Person 14\", \"Person 21\", \"Person 22\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719537d6-48f1-47f5-b1df-5c71c1267986",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d73265-57e4-4455-8f62-f648d67bb8d5",
   "metadata": {},
   "source": [
    "## Combining datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3810399-caf6-4171-a257-3ee26183adc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#warning will be thrown at the bottom, this is okay.\n",
    "\n",
    "#ensure TIME column is correctly formatted without changing timezone\n",
    "chatdata_sorted[\"TIME\"] = pd.to_datetime(chatdata_sorted[\"TIME\"], format=\"%d-%b-%y %I.%M.%S.%f %p\")\n",
    "\n",
    "#rename column to match normalized_df timestamp_gmt8\n",
    "chatdata_sorted = chatdata_sorted.rename(columns={\"TIME\": \"timestamp_gmt8\"})\n",
    "\n",
    "#ensure both timestamp_gmt8 columns are tz-naive (removes timezone for sorting compatibility)\n",
    "normalized_df[\"timestamp_gmt8\"] = normalized_df[\"timestamp_gmt8\"].dt.tz_localize(None)\n",
    "chatdata_sorted[\"timestamp_gmt8\"] = chatdata_sorted[\"timestamp_gmt8\"].dt.tz_localize(None)\n",
    "\n",
    "#merge chatdata_sorted with normalized_df using context.registration and USER\n",
    "merged_chat = chatdata_sorted.merge(\n",
    "    normalized_df[['actor.name', 'context.registration']], \n",
    "    left_on=\"USER\", \n",
    "    right_on=\"context.registration\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "#combine both datasets with specific columns\n",
    "final_combined_df = pd.concat([\n",
    "    normalized_df[['actor.name', 'context.registration', 'timestamp_gmt8', 'object.definition.name.und', 'verb.display.en-US', 'result.duration', 'slide_number']],  # Original actor data\n",
    "    merged_chat[['actor.name', 'context.registration', 'timestamp_gmt8', 'ROLE', 'CONTENT', 'USER']]  # Merged chat data\n",
    "], ignore_index=True)\n",
    "\n",
    "#ensure timestamps are tz-naive before sorting\n",
    "final_combined_df[\"timestamp_gmt8\"] = final_combined_df[\"timestamp_gmt8\"].dt.tz_localize(None)\n",
    "\n",
    "#sort everything by actor.name and timestamp_gmt8 to correctly slot in chat data\n",
    "final_combined_df = final_combined_df.sort_values(by=[\"actor.name\", \"timestamp_gmt8\"])\n",
    "\n",
    "# **Remove duplicate timestamps** by keeping only the first instance per actor & timestamp\n",
    "final_combined_df = final_combined_df.drop_duplicates(subset=[\"actor.name\", \"timestamp_gmt8\"], keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2e928d-0fe2-451d-b340-6de3d807ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combined_df[final_combined_df[\"actor.name\"] == \"Person 3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4058fc3-3347-42fe-96fa-3a554eac40e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combined_df = final_combined_df[final_combined_df['actor.name'].notna() & (final_combined_df['actor.name'] != '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b76da31-718e-4987-9b0e-5229e4c94729",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f5c9c7-0b02-45f2-8102-170360c818f6",
   "metadata": {},
   "source": [
    "#when wanting to view date in a proper format in excel\n",
    "final_combined_df[\"timestamp_gmt8\"] = \"'\" + final_combined_df[\"timestamp_gmt8\"].astype(str)\n",
    "\n",
    "final_combined_df.to_csv(\"StudentDataFeb11-21.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2dc6c5-1bc9-4e6e-a4da-508f51184eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combined_df.to_csv(\"ShinyFinalDataJuly25.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
